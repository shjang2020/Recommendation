# ğŸ“— Paper

ì¶”ì²œ ì‹œìŠ¤í…œ ë° ê´€ë ¨ ë¶„ì•¼ì˜ ë…¼ë¬¸ì„ ì½ê³  ë¦¬ë·°í•˜ë©° êµ¬í˜„í•˜ëŠ” ê³µê°„ì…ë‹ˆë‹¤.

## ğŸ“– ë…¼ë¬¸ ëª©ë¡

| ë°œí–‰ì¼ | ì œëª© | ë¶„ì•¼ | ë¦¬ë·° | êµ¬í˜„ |
|:---:|:---:|:---:|:---:|:---:|
| 2019 | [Finding Users Who Act Alike : Transfer Learning for Expanding Advertiser Audiences](https://www.pinterestlabs.com/media/phkg2uau/transferlearning-kdd2019.pdf) | ì¶”ì²œì‹œìŠ¤í…œ | [Notion](https://roasted-rake-be8.notion.site/Finding-Users-Who-Act-Alike-Transfer-Learning-for-Expanding-Advertiser-Audiences-1dc818aea60f80c0a738e856a4b1dfb2) | [ë°”ë¡œê°€ê¸°](./Finding%20Users%20Who%20Act%20Alike_Transfer%20Learning%20for%20Expanding%20Advertiser%20Audiences)|
| 2012 | [AlexNet: ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) | CV | [Notion](https://roasted-rake-be8.notion.site/ImageNet-Classification-with-Deep-Convolutional-Neural-Networks-2012-1fb818aea60f80649988cba3b9c695aa?source=copy_link) | - |
| 2015 | [Deep Residual Learning for Image Recognition (ResNet)](https://arxiv.org/abs/1512.03385) | CV | [PDF](./Deep%20Residual%20Learning%20for%20Image%20Recognition%20(ResNet)/Deep_Residual_Learning_for_Image_Recognition_(ResNet)%20ë…¼ë¬¸%20ë¦¬ë·°.pdf) | - |
| 2017 | [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | NLP | [Notion](https://www.notion.so/Attention-is-all-you-need-2017-205818aea60f80d986aed29772bbc9ff) | - |

## ì•ìœ¼ë¡œ ì½ì„ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸

| ë¶„ì•¼ | ë…¼ë¬¸ ì œëª© | ë°œí–‰ì—°ë„ | ì›ë¬¸ ë§í¬ |
|:---:|:---|:---:|:---:|
| ì¶”ì²œ ì‹œìŠ¤í…œ | Matrix Factorization Techniques for Recommender Systems | 2009 | [ì›ë¬¸](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf) |
| ì¶”ì²œ ì‹œìŠ¤í…œ | Deep Neural Networks for YouTube Recommendations | 2016 | [ì›ë¬¸](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/45530.pdf) |
| ì¶”ì²œ ì‹œìŠ¤í…œ | BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer | 2019 | [ì›ë¬¸](https://arxiv.org/pdf/1904.06690.pdf) |
| í˜„ëŒ€ì /ì‹¤ë¬´ì  ë°©ë²•ë¡  ë…¼ë¬¸ | OPT: Open Pre-trained Transformer Language Models | 2022 | [ì›ë¬¸](https://arxiv.org/pdf/2205.01068) |
| í˜„ëŒ€ì /ì‹¤ë¬´ì  ë°©ë²•ë¡  ë…¼ë¬¸ | TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis | 2023 | [ì›ë¬¸](https://arxiv.org/pdf/2210.02186) |
| í˜„ëŒ€ì /ì‹¤ë¬´ì  ë°©ë²•ë¡  ë…¼ë¬¸ | Lion: Symbolic Discovery of Optimization Algorithms | 2023 | [ì›ë¬¸](https://arxiv.org/pdf/2302.06675) |
| í˜„ëŒ€ì /ì‹¤ë¬´ì  ë°©ë²•ë¡  ë…¼ë¬¸ | Mamba: Linear-Time Sequence Modeling with Selective State Spaces | 2024 | [ì›ë¬¸](https://arxiv.org/pdf/2312.00752) |
| ìµœì‹  íŠ¸ë Œë“œ ë° ë©€í‹°ëª¨ë‹¬/LLM ë…¼ë¬¸ | Why Larger Language Models Do In-context Learning Differently? | 2024 | [ì›ë¬¸](https://arxiv.org/pdf/2405.19592) |
| ìµœì‹  íŠ¸ë Œë“œ ë° ë©€í‹°ëª¨ë‹¬/LLM ë…¼ë¬¸ | The Llama 3 Herd of Models | 2024 | [ì›ë¬¸](https://arxiv.org/pdf/2407.21783) |
| ìµœì‹  íŠ¸ë Œë“œ ë° ë©€í‹°ëª¨ë‹¬/LLM ë…¼ë¬¸ | Gemma: Open Models Based on Gemini Research and Technology | 2024 | [ì›ë¬¸](https://arxiv.org/pdf/2403.08295) |
| ìµœì‹  íŠ¸ë Œë“œ ë° ë©€í‹°ëª¨ë‹¬/LLM ë…¼ë¬¸ | Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference | 2024 | [ì›ë¬¸](https://arxiv.org/pdf/2403.04132) |

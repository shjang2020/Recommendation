# ğŸ“— Paper

ì¶”ì²œ ì‹œìŠ¤í…œ ë° ê´€ë ¨ ë¶„ì•¼ì˜ ë…¼ë¬¸ì„ ì½ê³  ë¦¬ë·°í•˜ë©° êµ¬í˜„í•˜ëŠ” ê³µê°„ì…ë‹ˆë‹¤.

## ğŸ“– ë…¼ë¬¸ ëª©ë¡

| ë¶„ì•¼ | ë°œí–‰ì¼ | ì œëª© | ë¦¬ë·° | êµ¬í˜„ |
|:---:|:---:|:---:|:---:|:---:|
| ì¶”ì²œì‹œìŠ¤í…œ | 2009 | Matrix Factorization Techniques for Recommender Systems | [Notion](https://roasted-rake-be8.notion.site/Matrix-Factorization-Techniques-for-Recommender-Systems-2009-209818aea60f8042b92ef38d52c217a2?source=copy_link) | [ë°”ë¡œê°€ê¸°](./Matrix%20Factorization%20Techniques%20for%20Recommender%20Systems/) |
| ì¶”ì²œì‹œìŠ¤í…œ | 2019 | Finding Users Who Act Alike : Transfer Learning for Expanding Advertiser Audiences | [Notion](https://roasted-rake-be8.notion.site/Finding-Users-Who-Act-Alike-Transfer-Learning-for-Expanding-Advertiser-Audiences-1dc818aea60f80c0a738e856a4b1dfb2) | [ë°”ë¡œê°€ê¸°](./Finding%20Users%20Who%20Act%20Alike_Transfer%20Learning%20for%20Expanding%20Advertiser%20Audiences/) |
| NLP | 2017 | Attention Is All You Need | [Notion](https://roasted-rake-be8.notion.site/Attention-is-all-you-need-2017-205818aea60f80d986aed29772bbc9ff?pvs=74) | - |
| CV | 2012 | ImageNet Classification with Deep Convolutional Neural Networks | [Notion](https://roasted-rake-be8.notion.site/ImageNet-Classification-with-Deep-Convolutional-Neural-Networks-2012-1fb818aea60f80649988cba3b9c695aa?source=copy_link) | - |
| CV | 2015 | Deep Residual Learning for Image Recognition (ResNet) | [Notion](https://roasted-rake-be8.notion.site/Deep-Residual-Learning-for-Image-Recognition-2015-205818aea60f80fe9544d15d53394b5d?pvs=74) | - |

## ì•ìœ¼ë¡œ ì½ì„ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸

| ë¶„ì•¼ | ë…¼ë¬¸ ì œëª© | ë°œí–‰ì—°ë„ | ì›ë¬¸ ë§í¬ |
|:---:|:---|:---:|:---:|
| ì¶”ì²œ ì‹œìŠ¤í…œ | Deep Neural Networks for YouTube Recommendations | 2016 | [ì›ë¬¸](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/45530.pdf) |
| ì¶”ì²œ ì‹œìŠ¤í…œ | BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer | 2019 | [ì›ë¬¸](https://arxiv.org/pdf/1904.06690.pdf) |
| í˜„ëŒ€ì /ì‹¤ë¬´ì  ë°©ë²•ë¡  ë…¼ë¬¸ | OPT: Open Pre-trained Transformer Language Models | 2022 | [ì›ë¬¸](https://arxiv.org/pdf/2205.01068) |
| í˜„ëŒ€ì /ì‹¤ë¬´ì  ë°©ë²•ë¡  ë…¼ë¬¸ | TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis | 2023 | [ì›ë¬¸](https://arxiv.org/pdf/2210.02186) |
| í˜„ëŒ€ì /ì‹¤ë¬´ì  ë°©ë²•ë¡  ë…¼ë¬¸ | Lion: Symbolic Discovery of Optimization Algorithms | 2023 | [ì›ë¬¸](https://arxiv.org/pdf/2302.06675) |
| í˜„ëŒ€ì /ì‹¤ë¬´ì  ë°©ë²•ë¡  ë…¼ë¬¸ | Mamba: Linear-Time Sequence Modeling with Selective State Spaces | 2024 | [ì›ë¬¸](https://arxiv.org/pdf/2312.00752) |
| ìµœì‹  íŠ¸ë Œë“œ ë° ë©€í‹°ëª¨ë‹¬/LLM ë…¼ë¬¸ | Why Larger Language Models Do In-context Learning Differently? | 2024 | [ì›ë¬¸](https://arxiv.org/pdf/2405.19592) |
| ìµœì‹  íŠ¸ë Œë“œ ë° ë©€í‹°ëª¨ë‹¬/LLM ë…¼ë¬¸ | The Llama 3 Herd of Models | 2024 | [ì›ë¬¸](https://arxiv.org/pdf/2407.21783) |
| ìµœì‹  íŠ¸ë Œë“œ ë° ë©€í‹°ëª¨ë‹¬/LLM ë…¼ë¬¸ | Gemma: Open Models Based on Gemini Research and Technology | 2024 | [ì›ë¬¸](https://arxiv.org/pdf/2403.08295) |
| ìµœì‹  íŠ¸ë Œë“œ ë° ë©€í‹°ëª¨ë‹¬/LLM ë…¼ë¬¸ | Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference | 2024 | [ì›ë¬¸](https://arxiv.org/pdf/2403.04132) |

# ğŸ“— Paper

ì¶”ì²œ ì‹œìŠ¤í…œ ë° ê´€ë ¨ ë¶„ì•¼ì˜ ë…¼ë¬¸ì„ ì½ê³  ë¦¬ë·°í•˜ë©° êµ¬í˜„í•˜ëŠ” ê³µê°„ì…ë‹ˆë‹¤.

## ğŸ“– ë…¼ë¬¸ ëª©ë¡

| ë°œí–‰ì¼ | ì œëª© | ë¦¬ë·° | êµ¬í˜„ |
|:---:|:---:|:---:|:---:|
| 2019.08 | [Finding Users Who Act Alike : Transfer Learning for Expanding Advertiser Audiences](https://www.pinterestlabs.com/media/phkg2uau/transferlearning-kdd2019.pdf) | [Notion](https://roasted-rake-be8.notion.site/Finding-Users-Who-Act-Alike-Transfer-Learning-for-Expanding-Advertiser-Audiences-1dc818aea60f80c0a738e856a4b1dfb2) | [ë°”ë¡œê°€ê¸°](./Finding%20Users%20Who%20Act%20Alike_Transfer%20Learning%20for%20Expanding%20Advertiser%20Audiences)|

## ì•ìœ¼ë¡œ ì½ì„ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸

### ì¶”ì²œ ì‹œìŠ¤í…œ (Recommendation Systems)
1. **Matrix Factorization Techniques for Recommender Systems** (2009) / [ì›ë¬¸](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf)
2. **Deep Neural Networks for YouTube Recommendations** (2016) / [ì›ë¬¸](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/45530.pdf)
3. **BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer** (2019) / [ì›ë¬¸](https://arxiv.org/pdf/1904.06690.pdf)

### ê¸°ì´ˆì™€ ëŒ€í‘œì  ë°©ë²•ë¡  ë…¼ë¬¸

#### ë”¥ëŸ¬ë‹Â·ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ êµ¬ì¡°ì™€ ë°œì „ íë¦„

1. **AlexNet**:
    - *ImageNet Classification with Deep Convolutional Neural Networks* (Krizhevsky et al., 2012)
    - ë”¥ëŸ¬ë‹(íŠ¹íˆ CNN) ë¶ì˜ ì‹œì‘ì„ ì•Œë¦° ë…¼ë¬¸. ì»´í“¨í„°ë¹„ì „ë¿ ì•„ë‹ˆë¼ ë”¥ëŸ¬ë‹ ì „ì²´ì˜ ëŒ€í‘œ ë…¼ë¬¸
    - [ì›ë¬¸](https://www.notion.so/ImageNet-Classification-with-Deep-Convolutional-Neural-Networks-2012-1fb818aea60f80649988cba3b9c695aa?pvs=21)

2. **ResNet**:
    - *Deep Residual Learning for Image Recognition* (He et al., 2015)
    - ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ê¹Šì´ë¥¼ ê·¹ì ìœ¼ë¡œ ëŠ˜ë¦´ ìˆ˜ ìˆê²Œ í•œ í˜ì‹ ì  êµ¬ì¡°.

3. **Attention is All You Need**:
    - (Vaswani et al., 2017)
    - íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡°ì˜ ì‹œì‘, ìì—°ì–´ì²˜ë¦¬ì™€ ë©€í‹°ëª¨ë‹¬ ë¶„ì•¼ì˜ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜ì 

### í˜„ëŒ€ì /ì‹¤ë¬´ì  ë°©ë²•ë¡  ë…¼ë¬¸

#### ëŒ€í‘œì  ìµœì‹  ë°©ë²•ë¡  ë° íŠ¸ë Œë“œ

1. **OPT: Open Pre-trained Transformer Language Models** (Meta, 2022)
    - ëŒ€í˜• ì–¸ì–´ëª¨ë¸(LLM) ì‹œëŒ€ë¥¼ ì—° ì˜¤í”ˆì†ŒìŠ¤ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ëŒ€í‘œ ë…¼ë¬¸. GPT-3ì™€ ë¹„êµí•´ë³¼ ë§Œí•¨

2. **TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis** (Wu et al., 2023)
    - ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ì— íŠ¹í™”ëœ ìµœì‹  ë”¥ëŸ¬ë‹ ëª¨ë¸

3. **Lion: Symbolic Discovery of Optimization Algorithms** (Chen et al., 2023)
    - ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ìµœì í™”(Optimizer) ë¶„ì•¼ í˜ì‹  ë…¼ë¬¸. Adamë³´ë‹¤ íš¨ìœ¨ì ì´ê³  ë©”ëª¨ë¦¬ ì ˆì•½í˜•

4. **Mamba: Linear-Time Sequence Modeling with Selective State Spaces** (2024)
    - ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„, íŠ¸ëœìŠ¤í¬ë¨¸ ì´í›„ ì£¼ëª©ë°›ëŠ” êµ¬ì¡°

### ìµœì‹  íŠ¸ë Œë“œ ë° ë©€í‹°ëª¨ë‹¬/LLM ë…¼ë¬¸

#### 2024ë…„ ê¸°ì¤€ ìµœì‹  íŠ¸ë Œë“œ ë…¼ë¬¸

1. **Why Larger Language Models Do In-context Learning Differently?** (Z. Shi et al., 2024)
    - LLMê³¼ SLMì˜ í•™ìŠµ ë°©ì‹ ì°¨ì´ì™€ ì¸ì»¨í…ìŠ¤íŠ¸ ëŸ¬ë‹ì˜ ì›ë¦¬ ë¶„ì„

2. **The Llama 3 Herd of Models** (Meta, 2024)
    - GPT-4ê¸‰ ì„±ëŠ¥ì˜ ë©€í‹°ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ëª¨ë¸, ìµœì‹  LLMì˜ ëŒ€í‘œ ë…¼ë¬¸

3. **Gemma: Open Models Based on Gemini Research and Technology** (Google, 2024)
    - Gemini ê³„ì—´ì˜ ìµœì‹  ì˜¤í”ˆì†ŒìŠ¤ LLM, ë©€í‹°ëª¨ë‹¬Â·ì±…ì„ì„± ë¶„ì„ í¬í•¨

4. **Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference** (2024)
    - LLMì˜ ì‹¤ì œ í‰ê°€ì™€ ë¹„êµë¥¼ ìœ„í•œ ì˜¤í”ˆ í”Œë«í¼ ë…¼ë¬¸

### ë…¼ë¬¸ ì„ ì •Â·í•™ìŠµ ì „ëµ

- **ê¸°ì´ˆ ë…¼ë¬¸**(AlexNet, ResNet, Transformer)ìœ¼ë¡œ ë…¼ë¬¸ êµ¬ì¡°ì™€ ìš©ì–´, ë°©ë²•ë¡ ì˜ íë¦„ì„ ìµíŒ ë’¤
- **ëŒ€í‘œì  ìµœì‹  ë°©ë²•ë¡ **(OPT, TimesNet, Lion, Mamba ë“±)ìœ¼ë¡œ ì‹¤ì „ íŠ¸ë Œë“œ ìŠµë“
- **ìµœì‹  íŠ¸ë Œë“œ ë…¼ë¬¸**(Llama 3, Gemma, Why LLMs Learn Differently ë“±)ìœ¼ë¡œ
    - LLM, ë©€í‹°ëª¨ë‹¬, ìµœì‹  ì‹œí€€ìŠ¤ ëª¨ë¸ë§, ìµœì í™” ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ í™•ì¥
- ë…¼ë¬¸ì„ ì½ì„ ë•ŒëŠ”
    - **Abstract/Conclusion/ê·¸ë¦¼/í‘œ** ë¨¼ì € íŒŒì•…
    - ëª¨ë¥´ëŠ” ìš©ì–´ë‚˜ ìˆ˜ì‹ì€ ë”°ë¡œ ì •ë¦¬
    - ì½”ë“œê°€ ê³µê°œëœ ë…¼ë¬¸ì€ ì§ì ‘ ì‹¤ìŠµ(êµ¬ê¸€ ì½œë© í™œìš©)
    - [ML Papers of the Week](https://github.com/dair-ai/ML-Papers-of-the-Week) ë“± ê¹ƒí—ˆë¸Œ íë ˆì´ì…˜ë„ ì°¸ê³ 

## ì°¸ê³  ìë£Œ

- [Matrix Factorization Techniques for Recommender Systems](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf)
- [Deep Neural Networks for YouTube Recommendations](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/45530.pdf)
- [BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer](https://arxiv.org/pdf/1904.06690.pdf)
- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
- [DistilBERT, a distilled version of BERT](https://arxiv.org/pdf/1910.01108.pdf)
- [The Google File System](https://static.googleusercontent.com/media/research.google.com/ko//archive/gfs-sosp2003.pdf)
- [MapReduce: Simplified Data Processing on Large Clusters](https://static.googleusercontent.com/media/research.google.com/ko//archive/mapreduce-osdi04.pdf)
- [Adam: A Method for Stochastic Optimization](https://arxiv.org/pdf/1412.6980.pdf)
- [Deep Learning with Limited Numerical Precision](https://arxiv.org/pdf/1502.02551.pdf) 